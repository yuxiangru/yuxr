
#基本的优化算法实现
#包括梯度下降法、投影梯度下降法、

import numpy as np

#BGD (batch gradient descent) 批量梯度下降

def BGD_batch():
    #训练样本
    x=np.array([(1, 1), (1, 2), (2, 2), (3, 1), (1, 3), (2, 4), (2, 3), (3,3)])
    y=np.array([3, 5, 6, 5, 7, 10, 8, 9])

    m,dim=x.shape
    theta=np.zeros(dim)
    alpha=0.01 #learning rate
    threshold=0.001
    iterations=1500 #最多迭代1500次终止迭代
    error=0

    #梯度的方向，指出了函数变化最快的方向,梯度的负方向是下降最快的方向
    for i in range(iterations):
        #为了求导方面，乘了一个1/2
        error=(1/(2*m)) * np.dot((np.dot(x,theta)-y).T,(np.dot(x, theta) - y))
        if error<=threshold:
            break
        #这里的np.dot(x.T, (np.dot(x,theta)-y))/m=error对theta求导
        #loss = (Ax-y).T *(Ax-y)/2*m
        #对应的导数是∂loss/∂A= x.T * (Ax-y) /m
        theta=theta-alpha*np.dot(x.T, (np.dot(x,theta)-y))/m
        print('多元变量：','迭代次数：%d', (i+1),'theta', theta,'error：%f',error)


#Note：2019年11月28日，当时写这段代码的时候犯的错误如下：
#忘记将y这个数组写成了y[j]，因为对于单样本而言，loss= (Ax[i]-y[i]).T * (Ax[i]-y[i])
def BGD_single():
    #训练样本
    x=np.array([(1, 1), (1, 2), (2, 2), (3, 1), (1, 3), (2, 4), (2, 3), (3,3)])
    y=np.array([3, 5, 6, 5, 7, 10, 8, 9])
    m,dim=x.shape
    theta=np.zeros(dim)
    alpha=0.01 #learning rate
    threshold=0.001
    iterations=1500 #最多迭代1500次终止迭代
    error=0

    for i in range(iterations):
        error=0
        error_single=0
        for j in range(m):
            # 对每个样本进行梯度更新
            error_single=(1/2)*np.dot((np.dot(x[j],theta)-y[j]).T,(np.dot(x[j],theta)-y[j]))
            error=error+error_single
            theta = theta - alpha * np.dot(x[j].T, (np.dot(x[j], theta) - y[j]))
        if (error/m)<=threshold:
            break
        print('多元变量：', '迭代次数：', (i + 1), 'theta', theta, 'error：', error)
BGD_single()
BGD_batch() 
